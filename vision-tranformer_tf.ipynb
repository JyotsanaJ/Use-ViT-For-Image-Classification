{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Transformers need to applied on patches and not pixels\n",
    "* If we use self-attention mechanism on image data, then each pixel in an image would need to attend and be compared to every other pixel. \n",
    "* The problem is, if we increase the pixel value by one, then the computational cost would increase quadratically. \n",
    "* This is simply not feasible if we have an image with a reasonably large resolution.\n",
    "\n",
    "1) Patching can be done by spliting the image into (no. of patches X (patch_size^2. channel)). Then apply linear layer to embed.\n",
    "2) Send it to a convolution layer with kernel_size and stride = patch_size. Then we flatten the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (50000, 32, 32, 3) - y_train: (50000, 1)\n",
      "x_test: (10000, 32, 32, 3) - y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train: {x_train.shape} - y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape} - y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72\n",
    "patch_size = 6  \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim * 2,projection_dim,] \n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(72, 72),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[3968.1458 3855.0076 4449.5435]]]], shape=(1, 1, 1, 3), dtype=float32)\n",
      "tf.Tensor([[[[125.306915 122.95039  113.86539 ]]]], shape=(1, 1, 1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(data_augmentation.layers[0].variance)\n",
    "print(data_augmentation.layers[0].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 31.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFICAYAAAAyFGczAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYjElEQVR4nO3dWW+c93XH8TPPbJyFw10kJVKyqN2bbMuy1TixAydAkbQpEPQuLZpe9abtO+hl30fTN1CgRVDEiZ06iZ2kcWAltuJFslaK4j4czgxnOOvTF1AcnJ+BAi3Q7+f64HCWZ358Lv7nOZk0TVMDAPw3yf/2CwCA/6sISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADhyauH5ly9LdZk0ztxu61jq1T5ohzWDzkDqNTe3INWdubAW1uzub0m9NjceSXXj0TCsOb3ylNSrVp0Ja+7feyL16vW0IavxuBMXZeLv0swsl4s/CzOzF567GtZMFKtSr9/9/pOwptluSb0yOe0zW1k7GdasXTon9er0+lLd/YfrYc3ItF6FahwdaV67/yqWy1JdJi3ERf281OsPP3pPquMOEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8iTN7Iw2lbC9vh3W1LcOpF75JD5hn2RLUq9KbU6qq00vhjU7de31D0cZqa5Sit/n9HRN6zURfx5JXphIMLM0q004jLtxTX4oTNuYWaarTUYd7OyHNdmC1utY+Jv5RPupZGws1aXH8cRNtqddP/0D4Qsws+xxPKU0vxpPYpmZWSV+n+1eT2qVyWp/ciwMWU1OTmnNRNxBAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwCEfFG8dHEp1g6P4cGhRPBmqHJPN5LSM741HUl2nHx/gHY20x7rn89pB69XV+PH7C3PaQfd+V1hnkWoHeOcW4tdlZjZRjNdZHGwVpV7Hbe0Qfr0TXx3FkXZQ3LLxNZTPiofrtY0LdtSMv4N0qP1Oxn3t2k4HwueR0Q66Z/Lx51+dqEi9jrramg1L4tfWH2nrXFTcQQKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAQ56kScbaJMFsNX5k+/bBltQrTeIJgc6wKfXKFlekuurUbFgzO6+d1h/0jrS6Qfw+F+a1SZrOYTzxlPa1qajG9m2p7sTZp8Oar/75X0u9jvvaVMWH770T1vRb2nU2TOMJk/5AXKUw1u45soV4NUa9oV0/rZa2zuLoKF7NUBU//5Pzq2HN8UjrlRS131PvOP6dNPfiVRxfBneQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcMgHxQsZrXRyeiqs6bW0R/63OvEB2LK0mMGssf1Eqvuo+bOwJpfVHoX/2qsvS3XDQXwguHGgHYh/5ukrYc1uvSH12t7XDpQfNuM1CR//7qbUa3LmhFS3unYhrGnvaGseOu1GWJMZaoe2ixXtd7J0Pl5TUVzUrrPJ2apUd1iMhz0Oum2p1/DRdlhTnJjQeonrUMql+H3mZrW/qeIOEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8iTNyUVtwiGx+PT/xhNtqmU0jh9zX8hqb2Hc1R7r3jy8F9aUy2Wp1zPnvyvVvfJHN8KarZ0dqdfyqZNhzY03viH1mihqExrrG5thzQ///S2p10/f/Q+p7vnrr4Y12dlFqdf9UT6sKWa0e4mnn48nfMzMqqcqYU1+Jn5dZmbVwrxUlz8V96uvt6ReWw/2wprZqXj9ipnZoBevvDAz62bjlRHlSe2aVXEHCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOfSdNXjvV3+v3w5pmpyH1GibxrorRWDuFn0m1/R65YrzH5Ovf/KbU6/U335TqVlZXw5pnnntO6nXYjneKHDTiHTJmZpO1SaluYTme5Dh/aU3qdeOrr0h1v3j/12HNZ7fjqSgzs8JEvNdo2Nd2Hx22td01vd1hXNTS7l8qtXgqx8wsb/EE2NryktSr9SDeF3XwUJv+UkOoK+yosvz/7D0fd5AA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwyAfF9+v7Ul1xaiKumYsPY5uZTU2UwpqJfFxjZvb4vrbm4fkXXg5rvvf970u9DppNqe7H//SDsCab1f6XXbhwMaw5tRqvZTAzqzd6Up2yGiMnDhrceOUFqW52Nn60/sEPNqReu1tpWFOuTUm9anOzUt361sOwZm9vV+q1OK8d7q5V4/ew3n4s9arvxYfAhy3t0Hwpp10buUI87FGd1b4nFXeQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOCQJ2l224dS3clT8ePfF58+JfVKsvHJ+UpOWwuw12pJdavnzoU1tbkFqdf7v3hPqvvF+78Ka+p72iTTKy/Hk0B/8Vd/KfUqCZNMZtqUVdqNV3GYmRWL2gqNy+fPhzV/9iffknrt7MfTI92xNhUys6RNcmTLp8Oavc0tqdeTu+tS3WDqOKw5eypen2FmdvZivCZkMqddP8dH2sTW1mGcQcOq9jdV3EECgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgEOepMlUtb0Rmcl4+qWci/eJmJllhT0mR/V4OsDMbHpFm35ZPB1P+RTL2mn9l4SpFjOz8Sje6bLxWNsVcu3Fl8KaEycWpV7jJH5dZmbFpXgP0WCgTcgMxbp8El+611+8JvX66NMPw5qf/udPpF4PHt2R6k7MnwhrVle03UHNTW3K7eJKfG1/5ysvSr3OzsS/gTPL8bSQmdn6Y233zj//y7+FNbe24105XwZ3kADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHDIB8XLiwWpLlMchjXZVDuA3G0KaxKG2lv49re/I9XdeOkrYU2r3ZR6tRsHUt20cPB8+cWrUq9Lz14Oa0qTRalXY2dbquv1umHN8rK2ZmM0TrW/KRwoLxS14YY3Xv96WLPf1j6LvYZ2UDlr8WsbxB+rmZmtLmkH/69fPRvWLM9oKyOmC/HrL2W1N5Ad16W6qXL8Wy9ltWtbxR0kADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADjkSZrjvjjhIByer+9okyh7jxthzdeufUPq9cLpV6W6pBtPCHS72oTA4a42SfPgiy/CmkuXL0i9cmkvrOk096Vetz69JdVlkvj/7MnT2uP3q+WKVJfr98OaXi/+LMzMzq9dDGsunn1a6tW6qa0/6O4fhTVpV/vN9cXJtM3NeLXBXKrdM506F3+f5ZH4+c9p0zt/fCNeYbJ1/Cupl4o7SABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADjkg+KttnYAtlyKW64unpR6XVmKD+cO9o+lXu/+69tSXaE4Edb0h+Kz8JP4MLOZ2XAYHxq2Ox2pV6fzJKwZjUdSr4197aD71ZfiA7xJISP1yhe1S3KcifuNtHPWNlWbD2uWZ1ekXnntbLTtPFgPa9Ij7QB4Jj8p1d17HH+fpax2zzRVEq6h4bTUazJXluoODhthTVLJSr1U3EECgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgEOepFk5M6s1TOLxhYmyNiGQG8RTMg+fPJJ6DaramofBOJ7QGBakVja9rD1KPs3H7zPX1yaG2nc3wppnzq9Jvb72glZ34fxSWDOZaCMm2XQg1aVJfOkep9pURWWiFtZcOhuvZTAzWz+rfWaLhfgi+uCDT6ReB/WGVNcalMKa2eUFqddNYW3KZlOYEDOzvfV4FYSZ2Ud3N8Oa9nT8XX4Z3EECgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgEOepBkPtNL9Vj2syZs2itLqxntYeuLel72Odlo/zcWTQMvnTku9DhNtj8/61oOwZiXVpnLmc/mwZjiYkXpdmNH2sMwO9uOierzrx8zMxEGIURrvMZnIVqReaTaeuLl29XmpV82Ez8LMHnz6UVgzXdJ2zTw+1KaP1hutsGZ3X5ty29uIr+1fHm5JvTpNbcrqeCDkxpG2b0nFHSQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8kHxQkE76JuO4sOcd24/kXpNVeKDslNL2mHacqUo1eVq8f+M0kK8lsHMLDPW/ubyRLyyoC0+vv7Jerxy4fbtL6Ref7gVH2Y2M3v9xRthzeWL16ReH9x6R6q7+Xm8amNp5ZzU68xTcd3VZ7WVC6fPPSvV1evtsKY6qw0azAsrO8zMDrvDsGZ3+0Dq1azHdbmCFi+5knagPyfczvV72qF5FXeQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOCQJ2kyifYo9mwunqTpdLU1CZNT8cTK1EpV6pWb0B7Fnhbjx++P8tpp/dZeU6qrC9MLSaJN5UwunQprcon2td8dat/5ldx8WHOhvCj1uv3w11Ldj96JJ25azR9KvSar8TW0uhJ/rmZmly5dluqWlpbDmt9+cl/q9ehhPFVkZjYex+tE+sN42sbMLM3GvSyJ13+YmfUHWh4Mx/FrS/LalJuKO0gAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcGTSNBWOxJvd+N5XpIZJNp6k6fe10/oji0/YL5ysSb3K5fh1mZllMvHESqer7QDZ3t2W6lrNVlgzFKda5mcXwppKeU7q9dTSFanu0txqWHPtorarpTo9LdV9eud2WPPTt96Wen34m1+FNbvbm1Ivy2j3HJVKfN1mc9r01DjV/mY2F0+2KHtfzMyGg15YMx5r16xqJPTLJtobePgovn7MuIMEABcBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAQz4o/ubffFNq2Gy2w5qcsJbBzCy1+KXli2LGa2/Tuu34cHqpVtF6jeLPwszMkngdxOLCktTqcKcR1rQb2mPpL57ThgOK3fg7SHramoqTT61IdVeeiw+x1/LaQeujRj2s+UA4TG5m9vOf/Vyq297ZC2vG2pYQy4nv0yz+3kfCAXAzsyQTH9pWD4pns/GaE7Uuk9Gu7QcPP5fquIMEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAEdOLcxk4wkTM7N+L54eOdzRVhb0juLVDN2mdvJ/NNJO9ZemSmHN4toJqVeupk0IpEk85ZMRHyV/5tRaWDO1tiz1Wjv1olQ3PIyvjd99+KHU67O7D6S6e1vxOou58qTU66mVeHrnpdfelHrlq7NS3S9/Hk/c1Hd3pF6NvXgqx8xs0I9/K+p1VpyYiGuK2oRPoaBN1vV68etXp3JU3EECgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAIR8Ur02VtbpKfDj3cFtbRXDv1qOw5ninK/XKZ7VDqzOL02FNpah9FpbX1jwM03gdweO7j6VeJ58+E9Z8641vSb1my9qB+KLlw5pKUTvAW+9p3+ex8Dj/tK/tLHi8vRvWfHLvvtTrsL4v1VWn4wPlQ+FgtJlZR1gZYWaWF37uY+2StVwu7qXUmJmNRtr3pBwUVw+dq7iDBAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABACHPEkzONZWFgxGwlH8Yvy4djMzy8XTF0kar2UwM8uPtLfa2o2nfHqmrZ+oLsfrG8zMBuO4X3P7SOo1dy2eflk9sSr1yohTFXlhYuLlV5+Xeg3H2lTF7Px8XCSOhezuN8KaTz6/J/W6+dvfSnX397bCmlZLmzizRLu2s/n4fiifEe+Z0kxcIq45GY6037AyJVMsab85FXeQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOCQJ2kso+10UfZQdLpNqVdjN961UUi1jM+m2lTF/u5OWFPOVaRey2tzUl1SrIY1M2VhcsTMajPxTqA0o004lCbESaB+PAlUKmm7Qra24gkTM7NLF86FNdVq/LmamS0txtNHS0sLUq9LZ09KdW+/9VZY88N1bXqnP9T2+CTCT2C6Ni31yibxHqLmkTYJNDbtt5kKv+HTp7UpMRV3kADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHDIB8Wbde0w6kQ+XqdQGGp/dnZiKqzpNhpSr0yqHY4uFOP/GdUJ7dB80eKVEWZmZeGg+MGRtnLhwfr9sKbRig/gm5lZvyaV7e/thzXdrnb9dDodqe7hvQdhzdramtTruBe/tmH3UOr11Gp86NzM7PXXXglr3n/3J1Kv7U1xBUgt/j5n52alXq3D+Hocj9U1LdrKhevXr4c1f/v3fyf1UnEHCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOeZLms5ufS3VzkzNhzaDdk3odt4UJh5F2Wj+baP8LckK7pKv9zfr9eMLEzGxzvB3W7O43pF7He6Ow5rm1q1Kvs8unpbp2M360/v7entRranpaqvvi89thTSGrXd61qXiFRrepTdKkvWOpbjwchDWjUfxdmpmVStoKkFRYT7Kx8UTqlQhTYsWiNnGWGYj3aZlMWFIql7VeIu4gAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcAhT9Lke1qWHhwK0yPi9MuoH08SpIWC1itNpbpkEP/N9lZT6tXcakh1o3hAwCrT2q6Qg82DsObmbz6Uem0vbEh1tepkWLO5uSn1mpmJJ7HMzBYWFsKa9997T+p15fLFsKY0kZd69cfCl2lmn9z6NKw57sTTNmZmuWy8B8rMbDSMfwPpWPydJPH7VCeBTPvI7NbHH4c1P37rx1KvN74W7wQy4w4SAFwEJAA4CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADvmg+NSk9lj3ozRekzDWzr/aeBg/vn4kPLrezCyT1Q7A5srxgeBKuST1yma0/z+HwsqCqvj5n714Pqx56fqLUq9O80iqe/zwYVhz94t4RYKZWS6JH+VvZra2Fr/PWk07dJ7NxT+Dc+fOSr3u3PlEqlvfiA/OJ+LKiEF/KNUlymeb0T7/sbC+QT0Bnoy1wZGh8Fv/2TtvS73sH/9BKuMOEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8iTN7v6OVDcexJlbmahJveYXToQ1w348bWNm1jpqSHULq/HfXFqOa8zMjtvaa2vfuRfWlGtlqdelKxfCmus3rkm9BkfahMa7vU5Y8/4v35V67e/vSXWtbjzl850//a7U69mrz4U1w5H2WSivy8zszr27YU39oC71Ege2zEyZWNGmX5TNDBlteM0y4n1aIrzR+/fuaH9UxB0kADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHPJB8UR6xLpZrTYd1szNLEi9pqamwpqjVkPqtf+FdtD9uBsf7s4l8VoGM7NRXzs0XCrGKxzmpuekXo16I6zptOKD3WZmJ6a172luLn5thcqE1OvwcUuqu33v87Dmyc5jqVep/FpcU5qUepXLBaluY+NRWDMY9KVe6kFr5RB4NtF6jUfxKfBxqp0Uz2hn082EbRDKAfYvgztIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHDIkzQTuXjaw8xsYS5eR1AsalMVg2E8SZAUtIw/s3ZGqqvNToc1czPzUq+ZctzLzGx+Ku5XLFWkXgf7jbBm4+GG1OuphVWpbm42nqSpTmmTKCvnTkt1w8EgrGl2DqReg1E8WZQXRzRabe1v5gvxWEiajqReylSLmVkiTNIkGe1vmvQntRGZNBHXPAivLdWG3GTcQQKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAQ56kGffFSYLDdljTycd7X8zMBuNeWJPJaSf/ixPaW202mmHN3fZdqVcpV5TqxsP4sz14oE2/jHPxhMZHN38v9brx3MtSXaUST/kME+36qcxpEzcThXj3y9LqotQrK6yRebIZ75AxM3vw4AupLp+P701SG0u9UnFfVEaYWElT7W9aRqnTXlcq7qRJhM9scWlJaybiDhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAO+aD4cb8r1fUOdsOa4Ug8jGrxIfATpxakTpmidmjb0vhAc7PdklrtHO5IdYnFh7szQo2ZWfMoPqj/8UfaQfGd3W2pbjwaxr2ebEq9ntS1uuWT8SHw4SgeNDAzO2rHwwH37t6Tet1/+FCqU9YMZBPt5zkWVxtkpDrtQL/yO8lIh8nNUvGkeJLEv4FEPDSv4g4SABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADgISAByZNBWOxAPA/0PcQQKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKA478ACvnkPPTsd44AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    " \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = int(np.sqrt(x_train.shape[1]))\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# for i, patch in enumerate(x_train[0]):\n",
    "#     ax = plt.subplot(n, n, i + 1)\n",
    "#     patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "#     plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    " \n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    print(augmented.shape)\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    print(patches)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches) # \n",
    " \n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    " \n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    logits = layers.Dense(10)(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    " \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "       keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "       keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
